import json
import requests
import openai
import os
import sounddevice as sd
import scipy.io.wavfile as wav
from openai import OpenAI
import subprocess
def llm_call(actions_list,  user_prompt):
    openai.api_key = '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'
    class ChatCompletionMessage:
        def __init__(self, content, role, function_call=None, tool_calls=None):
            self.content = content
            self.role = role
            self.function_call = function_call
            self.tool_calls = tool_calls

        def get_text_content(self):
            # Return only the text content
            return self.content
        # Initialize the OpenAI client
    client = openai.OpenAI()

    # Your code for interacting with the OpenAI API goes here
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": """Your job is to take in a user's math sentence and use your knowledge of Manim (gathered from this list: {actions_list}) and return fully functional Manim code with no placeholders. Please note your output will be parsed as raw code through Manim without any chance to adjust it, so do not leave anything unfinished. Also, u cannto import *, u most instead import the relevant modules or whatever u need. saying from manim import * will break the code, so u are not allowed to use asterisks in ur response. MaKE sure u have scene.render() at the end. Manim 0.18.0""" + """Here is an example of functioning manim 0.18.0 code:
            from manim import Scene, Square, PI, Create, Rotate

class RotateSquare(Scene):
    def construct(self):
        # Create a square
        square = Square()
        # Animate the creation of the square
        self.play(Create(square))
        # Rotate the square by 90 degrees
        self.play(Rotate(square, PI/2))
        self.wait(1)

# To render the scene
scene = RotateSquare()
scene.render()
"""},   
            {"role": "user", "content": user_prompt}
        ]
    )


    # Extract the actual content generated by GPT-3.5 Turbo
    generated_message = completion.choices[0].message
    generated_message_object = ChatCompletionMessage(
        content=generated_message.content,
        role='assistant'  # Assuming the role is 'assistant'
    )

    # Print out the text that will be read
    text_content = generated_message_object.get_text_content()
    print(text_content)
    return text_content

def llm_call2(user_prompt):
    openai.api_key = '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'
    class ChatCompletionMessage:
        def __init__(self, content, role, function_call=None, tool_calls=None):
            self.content = content
            self.role = role
            self.function_call = function_call
            self.tool_calls = tool_calls

        def get_text_content(self):
            # Return only the text content
            return self.content
        # Initialize the OpenAI client
    client = openai.OpenAI()

    # Your code for interacting with the OpenAI API goes here
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "Your job is to fix this ode for manim 0.18.0 and make it work perfectly. Ur response will be parsed as raw code so do not say anything else. make sure ur render is the same function as ur first scene."},
            {"role": "user", "content": user_prompt}
        ]
    )


    # Extract the actual content generated by GPT-3.5 Turbo
    generated_message = completion.choices[0].message
    generated_message_object = ChatCompletionMessage(
        content=generated_message.content,
        role='assistant'  # Assuming the role is 'assistant'
    )

    # Print out the text that will be read
    text_content = generated_message_object.get_text_content()
    print(text_content)
    return text_content

def record_audio(duration=5, fs=44100, filename='recorded_audio.wav'):
    print("Recording audio...")
    # Record audio for the specified duration and at the specified sampling frequency
    audio_data = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='float64')
    sd.wait()  # Wait for recording to finish
    print("Recording finished.")

    # Save the recorded audio to a WAV file
    wav.write(filename, fs, audio_data)
    print(f"Audio saved as {filename}")

    return filename

# Example usage:
# record_audio()

def whisper(audio):

    client = OpenAI()

    audio_file= open(audio, "rb")
    transcription = client.audio.transcriptions.create(
      model="whisper-1", 
      file=audio_file
    )
    return transcription.text 
def output_maker(code):
    import os
    import datetime

    # Get the current timestamp
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

    # Create the output directory if it doesn't exist
    output_folder = os.path.join('output', timestamp)
    os.makedirs(output_folder, exist_ok=True)

    # Define the file path
    file_path = os.path.join(output_folder, 'generated_code.py')

    # Write the variable to the file with timestamp
    with open(file_path, 'w') as file:
        # Indent the provided code properly
        file.write(code)

    print(f'Variable has been written to {file_path} with timestamp.')
    return file_path, output_folder


def main():
    record_audio()
    math_prompt = whisper("recorded_audio.wav")
    with open(r'C:\Users\Rose-\Desktop\Projects\Current\ManimAI\AIactionslist\output_1.txt', 'r') as file:
        actions_list = file.read()
    manim_output = llm_call2(llm_call(actions_list, math_prompt))
    manim_output_stripped = manim_output.strip().strip('`').replace('python', '')

# Assuming output_maker() function takes two arguments and returns file_path and output_folder
    file_path, output_folder = output_maker(manim_output_stripped)

    subprocess.run(['python', file_path])
main()